{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt', 'r') as f:\n",
    "    openai.api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a request with ada\n",
    "# response = openai.Completion.create(\n",
    "#     engine='ada',\n",
    "#     prompt='This is a test',\n",
    "#     temperature=0.6,\n",
    "#     max_tokens=5,\n",
    "#     top_p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.ChatCompletion.create(\n",
    "  model=GPT_MODEL,\n",
    "  messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'Who won the world series in 2020?'},\n",
    "        {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'},\n",
    "        {'role': 'user', 'content': 'Where was it played?'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generating research ideas\n",
    "1. Zero shot ask for ideas\n",
    "2. Feed it papers, and ask for ideas\n",
    "\n",
    "Generate a plan for a project\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDEA_SYSTEM_PROMPT = \\\n",
    "'You are GPT-Research and your purpose is to generate high quality, ML research ideas. ' + \\\n",
    "'You should generate ideas based on the user\\'s areas of preference. ' + \\\n",
    "'If the ideas fall into multiple areas, that is ideal, but not necessary. ' + \\\n",
    "'Research ideas should fit the following criteria:\\n' + \\\n",
    "'1. The idea should be novel and focused around understanding something new\\n' + \\\n",
    "'2. Proof of concept experiments should be relatively simple to implement, ' + \\\n",
    "'not requiring exessive compute or large codebases\\n' + \\\n",
    "'3. Be very specific and not too general. Niche is good.\\n' + \\\n",
    "'4. Not require external tools/data to test.\\n' + \\\n",
    "'5. Be focused on fundemental issues and understanding.\\n' + \\\n",
    "'Ideas should be listed as I1. {IDEA 1}\\nI2. {IDEA 2}, etc. ' + \\\n",
    "'Start directly with \"I1.\", no fluff.'\n",
    "\n",
    "PROJECT_SYSTEM_PROMPT = \\\n",
    "'You are GPT-Research and you generate research project outlines from ideas. ' + \\\n",
    "'The project outline should specify the different things that need to be coded ' + \\\n",
    "'and the experiments that need to be run to get to the point where the idea ' + \\\n",
    "'could be evaluated and potentially published. ' + \\\n",
    "'The project outline should be specific enough that a new ' + \\\n",
    "'grad student could implement it. It should be very detailed. ' + \\\n",
    "'The steps should be formatted as: S1. {STEP 1}\\nS2. {STEP 2}, etc.\\n' + \\\n",
    "'If some parts of the project will be dependent on other parts, ' + \\\n",
    "'only include the steps that are independent, then add a note ' + \\\n",
    "'at the end of the response in the format: \"FOLLOW UP WORK: ' + \\\n",
    "'{HIGH-LEVEL EXPLANATION OF FUTURE DIRECTION BASED ON RESULTS}. ' + \\\n",
    "'Start directly with \"S1.\", no fluff.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ideas(areas_of_interest, n=5, details=None, history=None):\n",
    "  \"\"\"\n",
    "  areas_of_interest: list of strings\n",
    "  \"\"\"\n",
    "  user_prompt = f'Generate {n} research ideas for the following areas of interest: ' + \\\n",
    "    ', '.join(areas_of_interest)\n",
    "  if details is not None:\n",
    "    user_prompt += f'. Details: {details}'\n",
    "  \n",
    "  if history is None:\n",
    "    history = [\n",
    "        {'role': 'system', 'content': IDEA_SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "      ]\n",
    "    \n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,\n",
    "    messages=history,\n",
    "    max_tokens=150,\n",
    "    stop='I2.'\n",
    "  )\n",
    "\n",
    "  response = response['choices'][0]['message'].to_dict()\n",
    "  history.append(response)\n",
    "  content = response['content']\n",
    "\n",
    "  return content, history\n",
    "\n",
    "def generate_outline(idea, details=None, history=None):\n",
    "  \"\"\"\n",
    "  idea: string\n",
    "  \"\"\"\n",
    "  user_prompt = f'Generate a project outline for the following idea:\\n{idea}'\n",
    "  if details is not None:\n",
    "    user_prompt += f'. Extra Details: {details}'\n",
    "  \n",
    "  if history is None:\n",
    "    history = [\n",
    "        # {'role': 'system', 'content': PROJECT_SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': PROJECT_SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "      ]\n",
    "    \n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,\n",
    "    messages=history,\n",
    "    max_tokens=400,\n",
    "    stop='S3.'\n",
    "  )\n",
    "\n",
    "  response = response['choices'][0]['message'].to_dict()\n",
    "  history.append(response)\n",
    "  content = response['content']\n",
    "\n",
    "  return content, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_of_interest = ['RL', 'model-based RL', 'representation learning', 'continual learning']\n",
    "details = None # 'I want ideas that focus on solving or understanding a problem'\n",
    "\n",
    "response, hist = generate_ideas(\n",
    "  areas_of_interest,\n",
    "  details=details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1. \"Exploration in model-based RL with predictive models\": Develop a new\n",
      "exploration algorithm for model-based RL by leveraging predictive models to\n",
      "improve the uncertainty estimates. This could be done by using variational\n",
      "methods to learn the approximate posterior over latent representations that are\n",
      "used to generate rollouts for the value and policy functions. Simple POC\n",
      "experiments could be done using classic RL environments like Atari games or\n",
      "robotic control tasks.  I\n"
     ]
    }
   ],
   "source": [
    "# Print response with text wrapping\n",
    "print(textwrap.fill(response, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "idea_idx = 1\n",
    "chosen_idea = response.split(f'I{idea_idx}. ')[1].split('I')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Exploration in model-based RL with predictive models\": Develop a new exploration algorithm for model-based RL by leveraging predictive models to improve the uncertainty estimates. This could be done by using variational methods to learn the approximate posterior over latent representations that are used to generate rollouts for the value and policy functions. Simple POC experiments could be done using classic RL environments like Atari games or robotic control tasks.\\n\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "details=None\n",
    "\n",
    "outline_response, outline_history = generate_outline(chosen_idea, details=details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  S1. Develop a predictive model that can generate rollouts for a value and\n",
      "policy function.  S2. Implement variational methods to learn the approximate\n",
      "posterior over latent representations.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(outline_response, 80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
